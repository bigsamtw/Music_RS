{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy.random import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "dir_ = './data/'\n",
    "file_name = 'normalized_filter_track_5_user_100.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(dir_, file_name[:-3] + 'pkl'))\n",
    "x_train = np.loadtxt(os.path.join(dir_, 'train_x_' + file_name), delimiter=',')\n",
    "x_test  = np.loadtxt(os.path.join(dir_, 'test_x_' + file_name), delimiter=',')\n",
    "y_train = np.loadtxt(os.path.join(dir_, 'train_y_' + file_name), delimiter=',')\n",
    "y_test  = np.loadtxt(os.path.join(dir_, 'test_y_' + file_name), delimiter=',')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.column_stack((x_train, y_train))\n",
    "test = np.column_stack((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ShuffleIterator(object):\n",
    "    \"\"\"\n",
    "    Randomly generate batches\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochIterator(ShuffleIterator):\n",
    "    \"\"\"\n",
    "    Sequentially generate one-epoch batches, typically for test data\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from math import sqrt, inf\n",
    "\n",
    "def loss_func_svd(rates, bias_global, embd_user, embd_item, bias_user, bias_item, penalty_rate):\n",
    "    regularizer = tf.reduce_sum(embd_user ** 2, 1) + tf.reduce_sum(embd_item ** 2, 1) + bias_user ** 2 + bias_item ** 2\n",
    "    penalty = tf.constant(penalty_rate, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "    cost = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "    cost = sum((rates - (bias_global+ bias_user + bias_item - cost)) ** 2)\n",
    "    l2_loss = cost + penalty * regularizer\n",
    "    return l2_loss, cost\n",
    "\n",
    "# load data\n",
    "data, train, test = load_movielens_100k(split_ratio = 0.9)\n",
    "\n",
    "def tf_svd():\n",
    "\n",
    "    BATCH_SIZE = 200\n",
    "    DIM = 5\n",
    "    EPOCH_MAX = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    PENALTY_RATE = 0.05\n",
    "    PATIENCE = 10\n",
    "\n",
    "    user_num = len(set(data['user']))\n",
    "    item_num = len(set(data['movie']))\n",
    "    num_batch = data.shape[0]/BATCH_SIZE\n",
    "    mini_vaild_cost = inf\n",
    "    patience = 1\n",
    "\n",
    "    iter_train = ShuffleIterator([train[\"user\"],\n",
    "                                  train[\"movie\"],\n",
    "                                  train[\"rating\"]],\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "\n",
    "    iter_test = OneEpochIterator([test[\"user\"],\n",
    "                                  test[\"movie\"],\n",
    "                                  test[\"rating\"]],\n",
    "                                 batch_size=-1)\n",
    "\n",
    "    bias_global = tf.Variable(initial_value = tf.zeros([1]), name = \"bias_global\")\n",
    "    w_bias_user = tf.Variable(initial_value = tf.zeros([user_num,]), name = \"embd_bias_user\")\n",
    "    w_bias_item = tf.Variable(initial_value = tf.zeros([item_num,]), name = \"embd_bias_item\")\n",
    "    w_user = tf.Variable(initial_value=tf.random.truncated_normal(shape=(user_num, DIM), stddev=0.02), name=\"embd_user\")\n",
    "    w_item = tf.Variable(initial_value=tf.random.truncated_normal(shape=(item_num, DIM), stddev=0.02), name=\"embd_item\")\n",
    "\n",
    "    test_user, test_item, test_rates = next(iter_test)\n",
    "    train_op = tf.optimizers.Adam(learning_rate = LEARNING_RATE)\n",
    "    test_user_tf = tf.constant(test_user, dtype=tf.int64, name='test_user_id')\n",
    "    test_item_tf = tf.constant(test_item, dtype=tf.int64, name='test_item_id')\n",
    "\n",
    "    print(\"{} {} {} {}\".format(\"epoch\", \"train_error\",  \"vaild_error\", \"elapsed_time\"))\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(int(EPOCH_MAX * num_batch)):\n",
    "        users, items, rates = iter_train.next()\n",
    "        user_tf = tf.constant(users, dtype=tf.int64, name='user_id')\n",
    "        item_tf = tf.constant(items, dtype=tf.int64, name='item_id')\n",
    "\n",
    "        with tf.GradientTape() as t:\n",
    "            embd_user = tf.nn.embedding_lookup(w_user, user_tf, name=\"embedding_user\")\n",
    "            embd_item = tf.nn.embedding_lookup(w_item, item_tf, name=\"embedding_item\")\n",
    "            bias_user = tf.nn.embedding_lookup(w_bias_user, user_tf, name=\"bias_user\")\n",
    "            bias_item = tf.nn.embedding_lookup(w_bias_item, item_tf, name=\"bias_item\")\n",
    "\n",
    "            l2_loss, loss = loss_func_svd(rates, bias_global, embd_user, embd_item, bias_user, bias_item, PENALTY_RATE)\n",
    "\n",
    "        grads = t.gradient(l2_loss, [bias_global, w_bias_user, w_bias_item, w_user, w_item])\n",
    "        train_op.apply_gradients(zip(grads, [bias_global, w_bias_user, w_bias_item, w_user, w_item]))\n",
    "\n",
    "\n",
    "        if i % BATCH_SIZE == 0:\n",
    "\n",
    "            test_embd_user = tf.nn.embedding_lookup(w_user, test_user_tf, name=\"test_embedding_user\")\n",
    "            test_embd_item = tf.nn.embedding_lookup(w_item, test_item_tf, name=\"test_embedding_item\")\n",
    "            test_bias_user = tf.nn.embedding_lookup(w_bias_user, test_user_tf, name=\"test_bias_user\")\n",
    "            test_bias_item = tf.nn.embedding_lookup(w_bias_item, test_item_tf, name=\"test_bias_item\")\n",
    "            test_cost = tf.reduce_sum(tf.multiply(test_embd_user, test_embd_item), 1)\n",
    "            test_cost = sum((test_rates - (bias_global + test_bias_user + test_bias_item - test_cost)) ** 2)\n",
    "            end = time.time()\n",
    "            print(\"{:3d} {:f} {:f} {:f}(s)\".format(i // BATCH_SIZE, sqrt(loss/bias_user.shape[0]),\n",
    "                                                   sqrt(test_cost/test_user_tf.shape[0]), end - start))\n",
    "\n",
    "            # early stopping\n",
    "            if test_cost < mini_vaild_cost:\n",
    "                mini_vaild_cost = test_cost\n",
    "                patience = 1\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= PATIENCE:\n",
    "                    print('no more improvment')\n",
    "                    break\n",
    "    return bias_global, w_bias_user, w_bias_item, w_user, w_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
